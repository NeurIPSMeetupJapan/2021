---
category: author-talk
datetime: 2021-12-13T14:50:00Z

title: Weighted Model Estimation for Offline Model-based Reinforcement Learning
speaker: Toru Hishinuma 菱沼 徹
affiliation: Kyoto University 京都大学
---

[[OpenReview]](https://openreview.net/forum?id=zdC5eXljMPy)
[[Conference]](https://neurips.cc/Conferences/2021/Schedule?showEvent=26612)
[[Proceeding]](https://proceedings.neurips.cc/paper/2021/hash/949694a5059302e7283073b502f094d7-Abstract.html)

**Toru Hishinuma**, Kei Senda

This paper discusses model estimation in offline model-based reinforcement learning (MBRL), which is important for subsequent policy improvement using an estimated model. From the viewpoint of covariate shift, a natural idea is model estimation weighted by the ratio of the state-action distributions of offline data and real future data. However, estimating such a natural weight is one of the main challenges for off-policy evaluation, which is not easy to use. As an artificial alternative, this paper considers weighting with the state-action distribution ratio of offline data and simulated future data, which can be estimated relatively easily by standard density ratio estimation techniques for supervised learning. Based on the artificial weight, this paper defines a loss function for offline MBRL and presents an algorithm to optimize it. Weighting with the artificial weight is justified as evaluating an upper bound of the policy evaluation error. Numerical experiments demonstrate the effectiveness of weighting with the artificial weight.
